{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Project: Named Entity Recognition using the NVIDIA BioMegatron model\n",
        "\n",
        "Objective: To finetune BioMegatron, a BERT-like Megatron-LM model pre-trained on a large biomedical text corpus (PubMed abstracts and full-text commercial use collection) - on the NCBI Disease Dataset for Named Entity Recognition."
      ],
      "metadata": {
        "id": "JADcXvH6WVQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install NeMo toolkit"
      ],
      "metadata": {
        "id": "yhALZFN-XVPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3EJGmi4WSU-"
      },
      "outputs": [],
      "source": [
        "# install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections import nlp as nemo_nlp\n",
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "import os\n",
        "import wget\n",
        "import torch\n",
        "import lightning.pytorch as pl\n",
        "from omegaconf import OmegaConf"
      ],
      "metadata": {
        "id": "UMUjAwiGXctX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task Description:\n",
        "\n",
        "Named entity recognition (NER), also referred to as entity chunking identification or extraction, is the task of detecting and classifying key information (entities) in text.\n",
        "\n",
        "Task: Given sentences from medical abstracts, what diseases are mentioned?\n",
        "\n",
        "Input: data input is sentences from the abstracts\n",
        "Output: the named disease entities in the abstract\n",
        "\n",
        "\n",
        "Dataset:\n",
        "\n",
        "\n",
        "The NCBI-disease corpus is a set of 793 PubMed abstracts, annotated by 14 annotators. The annotations take the form of HTML-style tags inserted into the abstract text using the clearly defined rules. The annotations identify named diseases, and can be used to fine-tune a language model to identify disease mentions in future abstracts, whether those diseases were part of the original training set or not."
      ],
      "metadata": {
        "id": "4OnPtTKEXhbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "\n",
        "DATA_DIR = \"DATA_DIR\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)"
      ],
      "metadata": {
        "id": "QPE3oQo7YNG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Downloading NCBI data...')\n",
        "wget.download('https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/NCBI_corpus.zip', DATA_DIR)\n",
        "! unzip -o {DATA_DIR}/NCBI_corpus.zip -d {DATA_DIR}"
      ],
      "metadata": {
        "id": "FS9Tk-k5YSZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Pre-process the dataset"
      ],
      "metadata": {
        "id": "80ED95cQYV-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NER_DATA_DIR = f'{DATA_DIR}/NER'\n",
        "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/train.tsv', NER_DATA_DIR)\n",
        "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/devel.tsv', NER_DATA_DIR)\n",
        "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/test.tsv', NER_DATA_DIR)"
      ],
      "metadata": {
        "id": "QbNBoJ-WYkt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh $NER_DATA_DIR"
      ],
      "metadata": {
        "id": "l9R0NbBRYmvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert these to a format that is compatible with NeMo Token Classification module, using the conversion script.\n",
        "\n",
        "! mv $NER_DATA_DIR/devel.tsv $NER_DATA_DIR/dev.tsv"
      ],
      "metadata": {
        "id": "b2VyCGMgYnJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/data/import_from_iob_format.py')\n",
        "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/train.tsv\n",
        "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/dev.tsv\n",
        "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/test.tsv"
      ],
      "metadata": {
        "id": "Nm5P2fjEYvJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NER task requires two files: the text sentences, and the labels. Run the next two cells to see a sample of the two files.\n"
      ],
      "metadata": {
        "id": "z6yXjtngYx-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head $NER_DATA_DIR/text_train.txt"
      ],
      "metadata": {
        "id": "vKFURhTwYyye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head $NER_DATA_DIR/labels_train.txt"
      ],
      "metadata": {
        "id": "iJcHkH8VY1a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IOB Tagging\n",
        "\n",
        "The abstract has been broken into sentences. Each sentence is then further parsed into words with labels that correspond to the original HTML-style tags in the corpus.\n",
        "\n",
        "The sentences and labels in the NER dataset map to each other with inside, outside, beginning (IOB) tagging. Anything separated by white space is a word, including punctuation. For the first sentence we have the following mapping:\n",
        "\n",
        "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
        "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
        "Recall the original corpus tags:\n",
        "\n",
        "Identification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor .\n",
        "The beginning word of the tagged text, \"adenomatous\", is now IOB-tagged with a B (beginning) tag, the other parts of the disease, \"polyposis coli tumour\" tagged with I (inside) tags, and everything else tagged as O (outside).\n",
        "\n",
        "Model configuration\n",
        "Our Named Entity Recognition model is comprised of the pretrained BERT model followed by a Token Classification layer.\n",
        "\n",
        "The model is defined in a config file which declares multiple important sections. They are:\n",
        "\n",
        "model: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
        "\n",
        "trainer: Any argument to be passed to PyTorch Lightning"
      ],
      "metadata": {
        "id": "4VULd2StY6Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
        "WORK_DIR = \"WORK_DIR\"\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "# download the model's configuration file\n",
        "config_dir = WORK_DIR + '/configs/'\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
        "else:\n",
        "    print ('config file is already exists')\n",
        "# this line will print the entire config of the model\n",
        "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
        "print(config_path)\n",
        "config = OmegaConf.load(config_path)\n",
        "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
        "config.model.train_ds.batch_size=8\n",
        "config.model.validation_ds.batch_size=8\n",
        "# in this tutorial train and dev datasets are located in the same folder, so it is enough to add the path of the data directory to the config\n",
        "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
        "\n",
        "# if you want to decrease the size of your datasets, uncomment the lines below:\n",
        "# NUM_SAMPLES = 1000\n",
        "# config.model.train_ds.num_samples = NUM_SAMPLES\n",
        "# config.model.validation_ds.num_samples = NUM_SAMPLES\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "metadata": {
        "id": "Hfn7Yl6iY3KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Model Training\n",
        "\n",
        "Setting up Data within the config\n",
        "\n",
        "The config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n"
      ],
      "metadata": {
        "id": "Hi1mddj0ZPg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n"
      ],
      "metadata": {
        "id": "5zVOlU8oZNGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the PyTorch Lightning Trainer\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem."
      ],
      "metadata": {
        "id": "3WaHM_5FZjnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "metadata": {
        "id": "KFUzDOJtZpH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.devices = 1\n",
        "config.trainer.accelerator = accelerator\n",
        "\n",
        "# for PyTorch Native AMP set precision=16\n",
        "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
        "\n",
        "# remove distributed training flags\n",
        "config.trainer.strategy = 'auto'\n",
        "\n",
        "trainer = pl.Trainer(**config.trainer)"
      ],
      "metadata": {
        "id": "OscvOXNfZtSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the NeMo"
      ],
      "metadata": {
        "id": "SoPe9HTHZ667"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "# the exp_dir provides a path to the current experiment for easy access\n",
        "exp_dir = str(exp_dir)\n",
        "exp_dir"
      ],
      "metadata": {
        "id": "AqKEokHlZ12x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
        "print([model.pretrained_model_name for model in MegatronBertModel.list_available_models()])\n",
        "\n",
        "\n",
        "config.model.language_model.lm_checkpoint = None\n",
        "config.model.language_model.pretrained_model_name = 'biomegatron345m_biovocab_30k_cased'\n",
        "config.model.tokenizer.tokenizer_name = None"
      ],
      "metadata": {
        "id": "jlzYncD6aDHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ner = nemo_nlp.models.TokenClassificationModel(cfg=config.model, trainer=trainer)"
      ],
      "metadata": {
        "id": "NLuqlEidaLeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring training progress"
      ],
      "metadata": {
        "id": "AZipHx0RawQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google import colab\n",
        "    COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir {exp_dir}\n",
        "else:\n",
        "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ],
      "metadata": {
        "id": "_R1W-lMgaw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start model training\n",
        "trainer.fit(model_ner)"
      ],
      "metadata": {
        "id": "RIgpPI0MazgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Inference"
      ],
      "metadata": {
        "id": "_HwMd4zCaYsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! head -n 100 $NER_DATA_DIR/text_dev.txt > $NER_DATA_DIR/sample_text_dev.txt\n",
        "! head -n 100 $NER_DATA_DIR/labels_dev.txt > $NER_DATA_DIR/sample_labels_dev.txt"
      ],
      "metadata": {
        "id": "xtllqAeoaYEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ner.half().evaluate_from_file(\n",
        "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
        "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
        "    output_dir=exp_dir,\n",
        "    add_confusion_matrix=False,\n",
        "    normalize_confusion_matrix=True,\n",
        "    batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "g1ZwqwYRaftf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}